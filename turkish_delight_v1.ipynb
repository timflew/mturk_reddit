{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Turkish delight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I've been running studies on [Mechanical Turk](https://www.mturk.com/mturk/welcome) ever since I entered grad school. A while back I Googled my first mturk and found some **HORRIBLE** reviews for my first HIT and since then I've made extra effort to write HITs with clear instructions that are fun (i.e., not dreadfully unpleasant) for subjects. \n",
    "\n",
    "In the same spirit, I wanted to see what sorts of positive and negative comments mturkers leave about our (mturk requestors) HITs and maybe give some general advice for making better tasks. My analyses are going to be based on the [mturk](https://www.reddit.com/r/mturk/) subreddit. Note, I didn't use other subreddits like HitsWorthTurkingFor because most of those posts are just brief descriptions of the HIT--By virtue of being on that subreddit these HITs are *already* good."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The game plan is:\n",
    "\n",
    "* Load and process (stem, lowercase) the text \n",
    "* Run a sentiment analysis so we can find the positive and negative comments\n",
    "* Figure out general topics of posts within those sentiments\n",
    "* Based on the negative topics, give some advice about how to make better HITs so we can all live harmoniously together. Because that's what the internet is alllllllll about"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First I'm going to load some packages. Typical stuff, but I'm going to use **praw** to scrape the **mturk** subreddit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import praw\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy.matlib import repmat\n",
    "import scipy as sp\n",
    "import os\n",
    "import pickle as pk\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And I'll set up my scraper and get some Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "user_agent = (pd.read_csv('r_id.csv',header=None)[0][0]) # made this weird for privacy\n",
    "\n",
    "r = praw.Reddit(user_agent = user_agent)\n",
    "subreddit = r.get_subreddit(\"mturk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fname='all_posts.pkl'\n",
    "if not os.path.exists(fname):\n",
    "    with open(fname,'wb') as post_file:\n",
    "        all_posts=[sbm for sbm in subreddit.get_hot(limit = 999)]\n",
    "        pk.dump(all_posts,post_file)\n",
    "else:\n",
    "    with open(fname,'rb') as post_file:\n",
    "        all_posts=pk.load(post_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_text=[]\n",
    "for ind,subm in enumerate(all_posts):\n",
    "    all_text.append(subm.selftext)\n",
    "\n",
    "all_data=pd.DataFrame({'raw_text':all_text})\n",
    "all_data=all_data.loc[all_data.raw_text!='',:].reset_index()\n",
    "all_data.drop('index',axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>raw_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Before posting to /r/mturk, please read the [F...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I am a researcher in Canada looking to ask a q...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Anyone else having issues with requester Trans...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>This is highly annoying and I know others expe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>There's some days where there's a really nice ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            raw_text\n",
       "0  Before posting to /r/mturk, please read the [F...\n",
       "1  I am a researcher in Canada looking to ask a q...\n",
       "2  Anyone else having issues with requester Trans...\n",
       "3  This is highly annoying and I know others expe...\n",
       "4  There's some days where there's a really nice ..."
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we have our posts!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process our text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And let's make the words a little more friendly for our machine learning analyses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "\n",
    "sw=stopwords.words('english')\n",
    "sb=SnowballStemmer(\"english\")\n",
    "valid_characters = string.ascii_letters + string.digits+string.whitespace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use **sw** to remove filler words like \"the\" and \"an\" and we'll us **sb** to reduce words to their roots, to facilitate matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def process_text(txt):\n",
    "    # get rid of punctuation\n",
    "    txt=''.join([i for i in txt if i in valid_characters])\n",
    "    \n",
    "    # split into words\n",
    "    txt_split=txt.lower().decode('latin-1').split(' ')\n",
    "    \n",
    "    # remove stopwords\n",
    "    txt_split=[i for i in txt_split if (not (i in sw))]\n",
    "    \n",
    "    # create stemmer to stem words\n",
    "    proc_text = ' '.join([sb.stem(i) for i in txt_split])\n",
    "    return proc_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is another version that doesn't stem words that I'm using to make the LDA a bit more interpretable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process_text2(txt):\n",
    "    # v2-got rid of stemming\n",
    "    \n",
    "    # get rid of punctuation\n",
    "    txt=''.join([i for i in txt if i in valid_characters])\n",
    "    \n",
    "    # split into words\n",
    "    txt_split=txt.lower().decode('latin-1').split(' ')\n",
    "    \n",
    "    # remove stopwords\n",
    "    txt_split=[i for i in txt_split if (not (i in sw))]\n",
    "    txt_split=[i for i in txt_split if (not (i in ['im','id','ive']))] # few other words that showed up a lot\n",
    "    proc_text = ' '.join(txt_split)\n",
    "    return proc_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_data.loc[:,'proc_text']=all_data.loc[:,'raw_text'].apply(process_text)\n",
    "all_data.loc[:,'proc_text2']=all_data.loc[:,'raw_text'].apply(process_text2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>raw_text</th>\n",
       "      <th>proc_text</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>proc_text2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Before posting to /r/mturk, please read the [F...</td>\n",
       "      <td>post rmturk pleas read faqhttpwwwredditcomrmtu...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>posting rmturk please read faqhttpwwwredditcom...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I am a researcher in Canada looking to ask a q...</td>\n",
       "      <td>research canada look ask question specif canad...</td>\n",
       "      <td>negative</td>\n",
       "      <td>researcher canada looking ask question specifi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Anyone else having issues with requester Trans...</td>\n",
       "      <td>anyon els issu request transcrib everi time ac...</td>\n",
       "      <td>negative</td>\n",
       "      <td>anyone else issues requester transcribe every ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>This is highly annoying and I know others expe...</td>\n",
       "      <td>high annoy know other experi much im look see ...</td>\n",
       "      <td>negative</td>\n",
       "      <td>highly annoying know others experience much lo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>There's some days where there's a really nice ...</td>\n",
       "      <td>there day there realli nice batch cant 80100 r...</td>\n",
       "      <td>negative</td>\n",
       "      <td>theres days theres really nice batch cant 8010...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            raw_text  \\\n",
       "0  Before posting to /r/mturk, please read the [F...   \n",
       "1  I am a researcher in Canada looking to ask a q...   \n",
       "2  Anyone else having issues with requester Trans...   \n",
       "3  This is highly annoying and I know others expe...   \n",
       "4  There's some days where there's a really nice ...   \n",
       "\n",
       "                                           proc_text sentiment  \\\n",
       "0  post rmturk pleas read faqhttpwwwredditcomrmtu...   neutral   \n",
       "1  research canada look ask question specif canad...  negative   \n",
       "2  anyon els issu request transcrib everi time ac...  negative   \n",
       "3  high annoy know other experi much im look see ...  negative   \n",
       "4  there day there realli nice batch cant 80100 r...  negative   \n",
       "\n",
       "                                          proc_text2  \n",
       "0  posting rmturk please read faqhttpwwwredditcom...  \n",
       "1  researcher canada looking ask question specifi...  \n",
       "2  anyone else issues requester transcribe every ...  \n",
       "3  highly annoying know others experience much lo...  \n",
       "4  theres days theres really nice batch cant 8010...  "
      ]
     },
     "execution_count": 259,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How can we identify positive vs. negative comments? I'm going to train a naive Bayes sentiment classifier on some sentiment-labeled Tweets I got a from [http://www.cs.york.ac.uk/semeval-2013/task2/](http://www.cs.york.ac.uk/semeval-2013/task2/) a while back. So let's get our sentiment data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id1</th>\n",
       "      <th>id2</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>264183816548130816</td>\n",
       "      <td>15140428</td>\n",
       "      <td>positive</td>\n",
       "      <td>Gas by my house hit $3.39!!!! I'm going to Cha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>264249301910310912</td>\n",
       "      <td>18516728</td>\n",
       "      <td>negative</td>\n",
       "      <td>Iranian general says Israel's Iron Dome can't ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>264105751826538497</td>\n",
       "      <td>147088367</td>\n",
       "      <td>positive</td>\n",
       "      <td>with J Davlar 11th. Main rivals are team Polan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>264094586689953794</td>\n",
       "      <td>332474633</td>\n",
       "      <td>negative</td>\n",
       "      <td>Talking about ACT's &amp;&amp; SAT's, deciding where I...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>254941790757601280</td>\n",
       "      <td>557103111</td>\n",
       "      <td>negative</td>\n",
       "      <td>They may have a SuperBowl in Dallas, but Dalla...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  id1        id2 sentiment  \\\n",
       "0  264183816548130816   15140428  positive   \n",
       "1  264249301910310912   18516728  negative   \n",
       "2  264105751826538497  147088367  positive   \n",
       "3  264094586689953794  332474633  negative   \n",
       "4  254941790757601280  557103111  negative   \n",
       "\n",
       "                                               tweet  \n",
       "0  Gas by my house hit $3.39!!!! I'm going to Cha...  \n",
       "1  Iranian general says Israel's Iron Dome can't ...  \n",
       "2  with J Davlar 11th. Main rivals are team Polan...  \n",
       "3  Talking about ACT's && SAT's, deciding where I...  \n",
       "4  They may have a SuperBowl in Dallas, but Dalla...  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_tweets=pd.read_csv('semval/tweet-b-actual.tsv',delimiter='\\t',header=None)\n",
    "all_tweets.columns=['id1','id2','sentiment','tweet']\n",
    "all_tweets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['negative', 'neutral', 'objective', 'objective-OR-neutral',\n",
       "       'positive'], dtype=object)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(all_tweets.sentiment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset has a few extra label types--we'll just be focusing on positive, negative and neutral. And while we're at it, we cand process our text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sel_sentiment=np.logical_or(np.logical_or(all_tweets.sentiment=='positive',all_tweets.sentiment=='negative'),all_tweets.sentiment=='neutral')\n",
    "sent_tweets=all_tweets.loc[sel_sentiment,['sentiment','tweet']]\n",
    "sent_tweets.loc[:,'proc_tweet']=sent_tweets.loc[:,'tweet'].apply(process_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>tweet</th>\n",
       "      <th>proc_tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>positive</td>\n",
       "      <td>Gas by my house hit $3.39!!!! I'm going to Cha...</td>\n",
       "      <td>gas hous hit 339 im go chapel hill sat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>negative</td>\n",
       "      <td>Iranian general says Israel's Iron Dome can't ...</td>\n",
       "      <td>iranian general say israel iron dome cant deal...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>positive</td>\n",
       "      <td>with J Davlar 11th. Main rivals are team Polan...</td>\n",
       "      <td>j davlar 11th main rival team poland hope make...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>negative</td>\n",
       "      <td>Talking about ACT's &amp;&amp; SAT's, deciding where I...</td>\n",
       "      <td>talk act  sat decid want go colleg appli colle...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>negative</td>\n",
       "      <td>They may have a SuperBowl in Dallas, but Dalla...</td>\n",
       "      <td>may superbowl dalla dalla aint win superbowl q...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  sentiment                                              tweet  \\\n",
       "0  positive  Gas by my house hit $3.39!!!! I'm going to Cha...   \n",
       "1  negative  Iranian general says Israel's Iron Dome can't ...   \n",
       "2  positive  with J Davlar 11th. Main rivals are team Polan...   \n",
       "3  negative  Talking about ACT's && SAT's, deciding where I...   \n",
       "4  negative  They may have a SuperBowl in Dallas, but Dalla...   \n",
       "\n",
       "                                          proc_tweet  \n",
       "0            gas hous hit 339 im go chapel hill sat   \n",
       "1  iranian general say israel iron dome cant deal...  \n",
       "2  j davlar 11th main rival team poland hope make...  \n",
       "3  talk act  sat decid want go colleg appli colle...  \n",
       "4  may superbowl dalla dalla aint win superbowl q...  "
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_tweets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pos_tweet=sent_tweets.loc[sent_tweets.sentiment=='positive',:].sample(800,replace=False)\n",
    "neg_tweet=sent_tweets.loc[sent_tweets.sentiment=='negative',:].sample(800,replace=False)\n",
    "neu_tweet=sent_tweets.loc[sent_tweets.sentiment=='neutral',:].sample(800,replace=False)\n",
    "sent_tweets_samp=pos_tweet.append(neg_tweet).append(neu_tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tweet_train_samp, tweet_test_samp, sent_train_samp, sent_test_samp = train_test_split(sent_tweets_samp.proc_tweet,sent_tweets_samp.sentiment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multinomial naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And let's train our classifier. The multinomial NB figures out, given the stemmed words in a post, what is the probability the post was positive, negative or neutral"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def nb_classifier(tweet_train, tweet_test, label_train, label_test,K,verbose=False):\n",
    "    # Run Naive Bayes classifier on binary presence\n",
    "    # Returns 1) word2vec transformation, 2) the nb classifier that turns the vector representation into a sentiment\n",
    "    # 3) training accuracy and 4) testing accuracy\n",
    "    # note, this requires the \n",
    "\n",
    "    # Vectorize training words\n",
    "    train_count_vec=CountVectorizer(binary=True)\n",
    "    train_count_words=train_count_vec.fit_transform(tweet_train)\n",
    "    test_count_words=train_count_vec.transform(tweet_test)\n",
    "    train_feat_names=train_count_vec.get_feature_names()\n",
    "\n",
    "    # Reduce feature space to K best features\n",
    "    ch2 = SelectKBest(chi2, k=K)\n",
    "    X_train = ch2.fit_transform(train_count_words, label_train)\n",
    "    X_test = ch2.transform(test_count_words)\n",
    "\n",
    "    feature_names=train_count_vec.get_feature_names()\n",
    "    k_feature_inds=[]\n",
    "    for ii in ch2.get_support(indices=True): \n",
    "        k_feature_inds.append(feature_names[ii])\n",
    "\n",
    "    # NB model\n",
    "    nb_class=MultinomialNB()\n",
    "    nb_class.fit(X_train,label_train)\n",
    "    \n",
    "\n",
    "    train_acc=nb_class.score(X_train,label_train)\n",
    "    test_acc=nb_class.score(X_test,label_test)\n",
    "    \n",
    "    if verbose:\n",
    "        print 'Training accuracy'\n",
    "        print train_acc\n",
    "        print 'Test accuracy'\n",
    "        print test_acc        \n",
    "\n",
    "    return train_count_vec,ch2,nb_class,train_acc,test_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try out a few different K values (number of diagnostic words according to the chi^2 selection process)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "k_range=[20*i for i in range(1,100)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1860\n",
      "0.543333333333\n"
     ]
    }
   ],
   "source": [
    "best_acc=0\n",
    "best_k=None\n",
    "best_count_vec=None\n",
    "best_multi_nb=None\n",
    "for K in k_range:\n",
    "    tcv,ch2,nb_class,train_acc,test_acc=nb_classifier(tweet_train_samp, tweet_test_samp, sent_train_samp, sent_test_samp,K,verbose=False)\n",
    "\n",
    "    if test_acc>best_acc:\n",
    "        clear_output()\n",
    "        best_k=K\n",
    "        best_count_vec=ch2\n",
    "        best_multi_nb=nb_class\n",
    "        best_acc=test_acc\n",
    "        print best_k\n",
    "        print best_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And let's retrain on all the data (ignore the test data. I left it there just to work with my function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.852021675698\n"
     ]
    }
   ],
   "source": [
    "tweet_train, tweet_test, sent_train, sent_test = train_test_split(sent_tweets_samp.proc_tweet,sent_tweets_samp.sentiment,test_size=.00001)\n",
    "tcv,ch2,nb_class,train_acc,test_acc=nb_classifier(tweet_train, tweet_test, sent_train, sent_test,best_k,verbose=False)\n",
    "print train_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reddit classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's apply our sentiment classifier to our reddit posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "reddit_cv=tcv.transform(all_data.proc_text) # transform to count vec\n",
    "reddict_ch2=ch2.transform(reddit_cv) # select chi2 best features\n",
    "reddit_sent=nb_class.predict(reddict_ch2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_data.loc[:,'sentiment']=reddit_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>raw_text</th>\n",
       "      <th>proc_text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Before posting to /r/mturk, please read the [F...</td>\n",
       "      <td>post rmturk pleas read faqhttpwwwredditcomrmtu...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I am a researcher in Canada looking to ask a q...</td>\n",
       "      <td>research canada look ask question specif canad...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Anyone else having issues with requester Trans...</td>\n",
       "      <td>anyon els issu request transcrib everi time ac...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>This is highly annoying and I know others expe...</td>\n",
       "      <td>high annoy know other experi much im look see ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>There's some days where there's a really nice ...</td>\n",
       "      <td>there day there realli nice batch cant 80100 r...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            raw_text  \\\n",
       "0  Before posting to /r/mturk, please read the [F...   \n",
       "1  I am a researcher in Canada looking to ask a q...   \n",
       "2  Anyone else having issues with requester Trans...   \n",
       "3  This is highly annoying and I know others expe...   \n",
       "4  There's some days where there's a really nice ...   \n",
       "\n",
       "                                           proc_text sentiment  \n",
       "0  post rmturk pleas read faqhttpwwwredditcomrmtu...   neutral  \n",
       "1  research canada look ask question specif canad...  negative  \n",
       "2  anyon els issu request transcrib everi time ac...  negative  \n",
       "3  high annoy know other experi much im look see ...  negative  \n",
       "4  there day there realli nice batch cant 80100 r...  negative  "
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYQAAAERCAYAAAB7FtAjAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1253eea10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "num_sent=all_data.groupby('sentiment').size().reset_index()\n",
    "num_sent.columns=['sentiment','size']\n",
    "fig=plt.figure()\n",
    "plt.bar([.1,1.1,2.1],num_sent.loc[:,'size'])\n",
    "plt.xticks([.5,1.5,2.5],[i.capitalize() for i in num_sent.sentiment])\n",
    "plt.xlim([-.5,3.5])\n",
    "plt.xlabel('Sentiment')\n",
    "plt.ylabel('Count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've got a lot of upset people posting on this Subreddit! Let's take a peek at what they're talking about"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anyone else having problems with Viral Mturk hits?  First two I tried had videos removed at YT.  Third and fourth hits I skipped two displayed blank urls in the hit page. \n",
      "\n",
      "*****************************\n",
      "Is this normal for survey's to ask you this.  I am pretty new and still learning the ropes.  I thought they were suppose to ask you any ID information.\n",
      "\n",
      "*****************************\n",
      "I didn't take it - My sister warned me first. Apparently, after completing the whole survey, it tells you that you are not in the right demographic. TO confirms, all her reviews mention the same thing.\n",
      "\n",
      "*****************************\n"
     ]
    }
   ],
   "source": [
    "for neg_post in all_data.loc[all_data.sentiment=='negative','raw_text'].sample(3):\n",
    "    print neg_post+'\\n'\n",
    "    print '*****************************'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the happy people?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What are the best ones I should be requesting?  Been turking for a few months but I often miss out on some because of qualifications that I'm yet to request.\n",
      "\n",
      "*****************************\n",
      "I recently applied to Mturk and was rejected. I am from Canada so I was kind of expecting to be rejected but I was just wondering how long I should wait until I reapply? If anyone has had any experience of being rejected but then reapplying and being accepted it would be greatly appreciated. \n",
      "\n",
      "Thank you.\n",
      "\n",
      "*****************************\n",
      "i have been turking since November, 2015 with 2684 approved HITs! i saw someone just reached his first grand with a little less than that number of HITs!! \n",
      "http://imgur.com/vxZN08u\n",
      "\n",
      "Learned lesson: stay in school!\n",
      "\n",
      "*****************************\n"
     ]
    }
   ],
   "source": [
    "for pos_post in all_data.loc[all_data.sentiment=='positive','raw_text'].sample(3):\n",
    "    print pos_post+'\\n'\n",
    "    print '*****************************'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, I'd say we're getting some signal out of this. The negative posts seem pretty negative and the positive posts, while not exuberant, do seem happier (or at least less aggressively negative, which I guess is what passes for positive on certain parts of the internet?)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But what are people in general saying in these posts? I'm going to run a Latent Dirichlet allocation topic model to get a nice summary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And here's a function to get our diagnostic features out of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_top_features(lda_model,tf_feature_names,n,verbose=False):\n",
    "    features_weights=lda_model.components_\n",
    "    top_features={}\n",
    "    for w_i,weights in enumerate(features_weights):\n",
    "        \n",
    "        best_features=[]\n",
    "        ranks=len(weights)-sp.stats.rankdata(weights)\n",
    "        for best in range(n):\n",
    "            feature=tf_feature_names[np.where(ranks==best)[0][0]]\n",
    "            best_features.append(feature)\n",
    "        if verbose:\n",
    "            print 'Component '+str(w_i)\n",
    "            print ', '.join(best_features)\n",
    "        top_features['Component '+str(w_i)]=best_features\n",
    "    top_features_df=pd.DataFrame(top_features).transpose()\n",
    "    top_features_df.loc[:,'topic']=range(len(features_weights))\n",
    "    return top_features_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General topic model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First let's run the topic model on all of our data together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>topic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Component 0</th>\n",
       "      <td>hit</td>\n",
       "      <td>survey</td>\n",
       "      <td>requester</td>\n",
       "      <td>want</td>\n",
       "      <td>like</td>\n",
       "      <td>got</td>\n",
       "      <td>rejected</td>\n",
       "      <td>surveys</td>\n",
       "      <td>rejection</td>\n",
       "      <td>minutes</td>\n",
       "      <td>bonus</td>\n",
       "      <td>people</td>\n",
       "      <td>dont</td>\n",
       "      <td>work</td>\n",
       "      <td>way</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Component 1</th>\n",
       "      <td>account</td>\n",
       "      <td>amazon</td>\n",
       "      <td>hit</td>\n",
       "      <td>email</td>\n",
       "      <td>payments</td>\n",
       "      <td>mturk</td>\n",
       "      <td>tried</td>\n",
       "      <td>bank</td>\n",
       "      <td>information</td>\n",
       "      <td>got</td>\n",
       "      <td>click</td>\n",
       "      <td>page</td>\n",
       "      <td>dont</td>\n",
       "      <td>try</td>\n",
       "      <td>message</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Component 2</th>\n",
       "      <td>hits</td>\n",
       "      <td>mturk</td>\n",
       "      <td>thanks</td>\n",
       "      <td>time</td>\n",
       "      <td>need</td>\n",
       "      <td>dont</td>\n",
       "      <td>work</td>\n",
       "      <td>know</td>\n",
       "      <td>guys</td>\n",
       "      <td>help</td>\n",
       "      <td>new</td>\n",
       "      <td>turk</td>\n",
       "      <td>available</td>\n",
       "      <td>like</td>\n",
       "      <td>wondering</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Component 3</th>\n",
       "      <td>hits</td>\n",
       "      <td>day</td>\n",
       "      <td>like</td>\n",
       "      <td>days</td>\n",
       "      <td>turking</td>\n",
       "      <td>hit</td>\n",
       "      <td>approved</td>\n",
       "      <td>10</td>\n",
       "      <td>time</td>\n",
       "      <td>batch</td>\n",
       "      <td>today</td>\n",
       "      <td>make</td>\n",
       "      <td>work</td>\n",
       "      <td>batches</td>\n",
       "      <td>know</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   0       1          2      3         4      5         6  \\\n",
       "Component 0      hit  survey  requester   want      like    got  rejected   \n",
       "Component 1  account  amazon        hit  email  payments  mturk     tried   \n",
       "Component 2     hits   mturk     thanks   time      need   dont      work   \n",
       "Component 3     hits     day       like   days   turking    hit  approved   \n",
       "\n",
       "                   7            8        9     10      11         12       13  \\\n",
       "Component 0  surveys    rejection  minutes  bonus  people       dont     work   \n",
       "Component 1     bank  information      got  click    page       dont      try   \n",
       "Component 2     know         guys     help    new    turk  available     like   \n",
       "Component 3       10         time    batch  today    make       work  batches   \n",
       "\n",
       "                    14  topic  \n",
       "Component 0        way      0  \n",
       "Component 1    message      1  \n",
       "Component 2  wondering      2  \n",
       "Component 3       know      3  "
      ]
     },
     "execution_count": 302,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_fname='all_lda.pkl'\n",
    "all_sent_text=all_data.proc_text2\n",
    "if not os.path.exists(all_fname):\n",
    "\n",
    "    n_features=100\n",
    "    tf_vectorizer = CountVectorizer(min_df=5, max_features=n_features,stop_words='english')\n",
    "    all_tf = tf_vectorizer.fit_transform(all_sent_text)\n",
    "\n",
    "    # Fit LDA on all test\n",
    "    all_lda_model=LatentDirichletAllocation(n_topics=4)\n",
    "    all_lda_model.fit(all_tf);\n",
    "    all_tf_feature_names = tf_vectorizer.get_feature_names()\n",
    "    \n",
    "    with open(all_fname,'wb') as post_file:\n",
    "        pk.dump((all_lda_model,all_tf_feature_names,all_tf),post_file)\n",
    "else:\n",
    "    with open(all_fname,'rb') as post_file:\n",
    "        all_lda_model,all_tf_feature_names,all_tf=pk.load(post_file)\n",
    "\n",
    "all_top_features=get_top_features(all_lda_model,all_tf_feature_names,15,verbose=False)   \n",
    "all_top_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This table tells us what words were most diagnostic for each of our components/topics. And I've ordered the words, 0->15, such that word 0 has the strongest influence and the influence of each word decreases subsequently. That being said, we have 100 features, so latter words in aggregate can play an important role"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Diving into our topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now let's get the topics of our posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_data.loc[:,'topic']=[np.argmax(i) for i in all_lda_model.transform(all_tf)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "topic\n",
       "0    205\n",
       "1    163\n",
       "2    249\n",
       "3    249\n",
       "dtype: int64"
      ]
     },
     "execution_count": 297,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data.groupby('topic').size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get a better idea of what our topics are, I'm going to find the posts that were most exemplary of each topic. So, for each posts I'm going to find the probability of it coming from each topic and then for each topic find the post that had the highest probability of belonging to it (vs. another topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "topics=np.unique(all_data.loc[:,'topic'])\n",
    "weights=all_lda_model.transform(all_tf)\n",
    "norm=repmat([ [i] for i in np.sum(weights,axis=1)],1,len(topics))\n",
    "best_id=np.argmax(weights/norm,axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topic 0: Requester looking for Requester"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at our first topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Component 0\n",
      "I have a survey that I recently posted on MTurk and will be posting again. The survey involves some free response questions that we really want people to take their time with and think about. In the last batch we posted, there were an alarming number of workers who didn't give adequate answers-- they were either way underdeveloped (i.e., three words where sentences were clearly called for), or fairly nonsensical (I suspected bots in this situations, though I suppose someone not very good at English could have produced some of these responses). We had a few attention checks that people missed too. \n",
      "\n",
      "My question is, for the next go, is there some way we can discourage the people who might not be willing to put in time and thought to this survey? We already mention in the HIT description that it will involve a writing exercise and ask that people not do it unless willing to engage in that, and I don't think that the survey being overlong or mind-numbingly boring is the problem (for one thing, the writing exercise comes early on, and most of the comments people leave in the allotted space are positive, saying it was an interesting study).\n",
      "\n",
      "Should we mention that there are attention checks and that people will be rejected for failing them to discourage sloppy survey takers? (We haven't currently been failing people for missing the attention checks, but could start). Should we mentioned that people will be rejected for submitting free-response answers that seem suspect? Should we specify that people must be fluent in English?\n",
      "\n",
      "Or would paying more help? Willing to do it if it would, but don't want to waste our limited resources if it won't end up making a difference... Especially since we simply want to discourage the bad eggs (for lack of a better term) from taking our survey. They non-bad eggs have been doing a great job.\n",
      "\n",
      "TLDR: How do I discourage sloppy/bot workers from taking my survey involving a writing exercise?\n",
      "\n",
      "*************************\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ind=0\n",
    "samp_post=all_data.iloc[best_id[ind],:]\n",
    "print 'Component '+str(ind)\n",
    "print str(samp_post.raw_text)\n",
    "print '\\n*************************\\n'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This topic seems to do mostly with posts from requesters who are trying to figure out how to make better studies. For example, looking at the feature words we see some mentions of **rejecting/rejection of** turkers, **minutes**, **bonuses**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topic 1: Blame it on Amazon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Component 1\n",
      "I made up mechanical turk account I filled up my information and address then amazon payments told me to enter my information I filled it all then they sent me that the vertification failed and my account is suspended I saw on reddit forum that I must fill my tax information  I filled it and until now status is pending I don't know if the status changed to accept my account could be unsuspended I made a new account until I see what will happen I made my account mechanical turk then I filled my current address and security question and agree then they send me emal to verify my email address I verified before I enter my amazon payments account information I edit the tax information today then they said that I work on hits until they verify my tax information when I click on accept hit they send me to amzon payments to enter my information so idont know now wait until the tax is verified to avoid account suspended or I fill my information in amzon payments they might verify it because I edit my tax information or not so idont know plz helpppp\n",
      "\n",
      "*************************\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ind=1\n",
    "samp_post=all_data.iloc[best_id[ind],:]\n",
    "print 'Component '+str(ind)\n",
    "print str(samp_post.raw_text)\n",
    "print '\\n*************************\\n'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pretty straightforward--people who have problems with the Mechanical Turk system instead of studies themselves. This person seems to have gone through quite the ringer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topic 2: How to HIT good"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Component 2\n",
      "And yes, I swear I searched before posting this! :)  \n",
      "It's my first time doing any of their HITs and I have a shoe question.  \n",
      "Rather, the bounding box question. It doesn't say if you are to draw the bounding box even if the shoe (or other item) is not what they are looking for.    \n",
      "\n",
      "Does anyone do enough of these to know? I submitted about 4, and didn't do the bounding box, so either I did 2 right and 2 wrong, either way.  \n",
      "\n",
      "Edit: Thanks for everyone's help. All were approved just fine.\n",
      "\n",
      "Gracias.\n",
      "\n",
      "*************************\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ind=2\n",
    "samp_post=all_data.iloc[best_id[ind],:]\n",
    "print 'Component '+str(ind)\n",
    "print str(samp_post.raw_text)\n",
    "print '\\n*************************\\n'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This topic may be HIT advice (honestly I'm a bit unsure)? The words like **need**, **work**, and **know** do make it seem somewhat skill based? Those words *are* in the next topic too, albeit lower ranked, but there they may reflect the qualifications necessary for certain HITs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topic 3: Spaceballs 2: The Search for More Money"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Component 3\n",
      "I've been turking for about 3 months now and recently started using hitscraper. I've definitely noticed an increase in my pay. I've been able to grab several very juicy hits that I doubt I would've gotten otherwise. My question: is there a way to make hitscraper auto accept hits? The best ones that come in I can only seem to get about 25% of the time. This is while I'm working on a hit in one tab I react to the beep and try to PANDA the new good one. If I don't get it, I put it into turkmaster and auto accept it. This ends yielding me the hit about 10% of the time. How should I use these two scripts together? I mostly do surveys. Can never seem to find profitable batches. \n",
      "\n",
      "*************************\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ind=3\n",
    "samp_post=all_data.iloc[best_id[ind],:]\n",
    "print 'Component '+str(ind)\n",
    "print str(samp_post.raw_text)\n",
    "print '\\n*************************\\n'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This topic seems to be about how mturkers can get HITs and more $$$, based on the presence of words like **day** and **batch**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Takeaway"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The topic model appears to be recovering some interesting categories. We've got requesters in one group, and then account, HIT and how to find HITs topics in their own groups. To some extent, these topics even seem to reflect the [tags in the subreddit](https://www.reddit.com/r/mturk/) like \"Requester help\" and \"Qual/HIT question\"\n",
    "\n",
    "And that brings us to our next analyses--what are negative/positive valence people posting about?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Negative sentiment topic model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's see what people like to complain about. I'm going to run an LDA on **just** the negative posts people wrote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>topic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Component 0</th>\n",
       "      <td>hits</td>\n",
       "      <td>like</td>\n",
       "      <td>survey</td>\n",
       "      <td>people</td>\n",
       "      <td>approved</td>\n",
       "      <td>use</td>\n",
       "      <td>surveys</td>\n",
       "      <td>using</td>\n",
       "      <td>100</td>\n",
       "      <td>turking</td>\n",
       "      <td>day</td>\n",
       "      <td>new</td>\n",
       "      <td>didnt</td>\n",
       "      <td>days</td>\n",
       "      <td>hit</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Component 1</th>\n",
       "      <td>hit</td>\n",
       "      <td>hits</td>\n",
       "      <td>time</td>\n",
       "      <td>tried</td>\n",
       "      <td>dont</td>\n",
       "      <td>work</td>\n",
       "      <td>click</td>\n",
       "      <td>mturk</td>\n",
       "      <td>says</td>\n",
       "      <td>message</td>\n",
       "      <td>new</td>\n",
       "      <td>try</td>\n",
       "      <td>page</td>\n",
       "      <td>doesnt</td>\n",
       "      <td>search</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Component 2</th>\n",
       "      <td>account</td>\n",
       "      <td>amazon</td>\n",
       "      <td>dont</td>\n",
       "      <td>know</td>\n",
       "      <td>payments</td>\n",
       "      <td>money</td>\n",
       "      <td>mturk</td>\n",
       "      <td>bank</td>\n",
       "      <td>need</td>\n",
       "      <td>really</td>\n",
       "      <td>right</td>\n",
       "      <td>didnt</td>\n",
       "      <td>say</td>\n",
       "      <td>start</td>\n",
       "      <td>able</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Component 3</th>\n",
       "      <td>got</td>\n",
       "      <td>hits</td>\n",
       "      <td>like</td>\n",
       "      <td>mturk</td>\n",
       "      <td>requester</td>\n",
       "      <td>hit</td>\n",
       "      <td>work</td>\n",
       "      <td>email</td>\n",
       "      <td>way</td>\n",
       "      <td>requesters</td>\n",
       "      <td>dont</td>\n",
       "      <td>time</td>\n",
       "      <td>make</td>\n",
       "      <td>really</td>\n",
       "      <td>information</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   0       1       2       3          4      5        6  \\\n",
       "Component 0     hits    like  survey  people   approved    use  surveys   \n",
       "Component 1      hit    hits    time   tried       dont   work    click   \n",
       "Component 2  account  amazon    dont    know   payments  money    mturk   \n",
       "Component 3      got    hits    like   mturk  requester    hit     work   \n",
       "\n",
       "                 7     8           9     10     11     12      13  \\\n",
       "Component 0  using   100     turking    day    new  didnt    days   \n",
       "Component 1  mturk  says     message    new    try   page  doesnt   \n",
       "Component 2   bank  need      really  right  didnt    say   start   \n",
       "Component 3  email   way  requesters   dont   time   make  really   \n",
       "\n",
       "                      14  topic  \n",
       "Component 0          hit      0  \n",
       "Component 1       search      1  \n",
       "Component 2         able      2  \n",
       "Component 3  information      3  "
      ]
     },
     "execution_count": 303,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neg_fname='neg_lda.pkl'\n",
    "neg_sent_data=all_data.loc[all_data.sentiment=='negative',:]\n",
    "neg_sent_text=neg_sent_data.proc_text2\n",
    "if not os.path.exists(neg_fname):\n",
    "\n",
    "    n_features=100\n",
    "    tf_vectorizer = CountVectorizer(min_df=5, max_features=n_features,stop_words='english')\n",
    "    neg_tf = tf_vectorizer.fit_transform(neg_sent_text)\n",
    "\n",
    "    # Fit LDA on all test\n",
    "    neg_lda_model=LatentDirichletAllocation(n_topics=4)\n",
    "    neg_lda_model.fit(neg_tf);\n",
    "    neg_tf_feature_names = tf_vectorizer.get_feature_names()\n",
    "    \n",
    "    with open(neg_fname,'wb') as post_file:\n",
    "        pk.dump((neg_lda_model,neg_tf_feature_names,neg_tf),post_file)\n",
    "else:\n",
    "    with open(neg_fname,'rb') as post_file:\n",
    "        neg_lda_model,neg_tf_feature_names,neg_tf=pk.load(post_file)\n",
    "\n",
    "neg_top_features=get_top_features(neg_lda_model,neg_tf_feature_names,15,verbose=False)   \n",
    "neg_top_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "neg_sent_data.loc[:,'topic']=[np.argmax(i) for i in neg_lda_model.transform(neg_tf)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "topic\n",
       "0    117\n",
       "1    165\n",
       "2     80\n",
       "3    168\n",
       "dtype: int64"
      ]
     },
     "execution_count": 309,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neg_sent_data.groupby('topic').size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "topics=np.unique(neg_sent_data.loc[:,'topic'])\n",
    "weights=neg_lda_model.transform(neg_tf)\n",
    "norm=repmat([ [i] for i in np.sum(weights,axis=1)],1,len(topics))\n",
    "best_id=np.argmax(weights/norm,axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topic 0: Turkers on probation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Component 0\n",
      "There's some days where there's a really nice batch that I can't do more than 80-100 and it's really annoying. Like the Horse Name one today.\n",
      "\n",
      "They should make it like that : \n",
      "\n",
      "- You are no longer in probation if you complete 100 or more hits and earn 30$ during the first 10 days. If you hit that goal during the second day, you are no longer in probation afterwards.\n",
      "\n",
      "Like I made 60$ in the past 7 days with 430 HITS approved. 98.9% Accuracy. Like comon, did I not gain your trust yet? :P\n",
      "\n",
      "*************************\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ind=0\n",
    "samp_post=neg_sent_data.iloc[best_id[ind],:]\n",
    "print 'Component '+str(ind)\n",
    "print str(samp_post.raw_text)\n",
    "print '\\n*************************\\n'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This one seems to reflect Turkers who, for whatever reason, got flagged. **day** seems to be indicative of how long they're on probation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topic 1: REQUESTERS DEBUG YOUR CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Component 1\n",
      "I try to submit my code and I get this error:\n",
      "\n",
      " There was a problem submitting your results for this HIT.\n",
      "\n",
      "This HIT is still assigned to you. To try this HIT again, click \"HITs Assigned To You\" in the navigation bar, then click \"Continue work on this HIT\" for the HIT. If this problem persists, you can contact the Requester for this HIT using the \"Contact\" link above.\n",
      "\n",
      "To return this HIT and continue working on other HITs, click the \"Return HIT\" button.\n",
      "\n",
      "\n",
      "I tried everything and it still won't let me hit submit.  Help?\n",
      "\n",
      "*************************\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ind=1\n",
    "samp_post=neg_sent_data.iloc[best_id[ind],:]\n",
    "print 'Component '+str(ind)\n",
    "print str(samp_post.raw_text)\n",
    "print '\\n*************************\\n'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok requesters, make sure your HITs work! Test them out in sandbox and when you do deploy them, keep an eye until the first few turkers complete your study. It took me a few HITs to really discipline myself on this. In the feature words, we can see things like **click**, **don't/doesn't** and **tried**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topic 2: Blame it on Amazon (redux)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Component 2\n",
      "I have around $150 that I earned Turking in the last week or so in my Amazon Payments account. In the past, I have used my Turking earnings to buy merchandise from Amazon.com. But now, I really need my money.\n",
      "\n",
      "I tried to set up my bank account with Amazon Payments. As part of their verification process they sent 2 small random deposits to my account. When I went to verify my account using the amounts of these deposits, they said they were unable to verify that I had control of the account I was attempting to add, \"FOR MY PROTECTION.\"\n",
      "\n",
      "I tried to send screenshots of the emails that I got from my bank informing me of the 2 deposits, but they rejected that info saying that screenshots were not acceptable, and that I needed to send a bank statement. Cool, because my bank statement just happened to arrive the next day, and YES, the two random deposits were actually on the bank statement. What more could you ask for?\n",
      "\n",
      "Well, I sent the bank statement. But the bad news is that my bank, \"FOR MY PROTECTION,\" does not put my bank account number on my statement. It seems they do that to prevent identity fraud. Now Amazon Payments won't validate my account, even though it is painfully obvious that it belongs to me, since my name and address match what Amazon Payments has on file, and since the 2 random deposits that Amazon Payments made to the account actually appear RIGHT THERE on the statement.\n",
      "\n",
      "It seems that Amazon Payments has set the bar for verifying my bank account impossibly high. And you know, I get it. They want me to spend my money on Amazon.com. I get it.\n",
      "\n",
      "However, as an independent contractor, one who gets 1099's from Amazon, I feel like they should try a little harder to help me get my money. They don't offer any other options, for example, they don't send paper checks. I suppose my only option is to find a bank that sends statements that include my account number right on the page, if such a bank even exists in this day of identity theft.\n",
      "\n",
      "Has anyone had similar issues? Does anyone have a suggestion? Any help appreciated.\n",
      "\n",
      "*************************\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ind=2\n",
    "samp_post=neg_sent_data.iloc[best_id[ind],:]\n",
    "print 'Component '+str(ind)\n",
    "print str(samp_post.raw_text)\n",
    "print '\\n*************************\\n'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "People who had trouble with Amazon (similar to the Amazon cluster in the previous aggregate analysis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topic 3: Cautionary tales of horrible requesters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Component 3\n",
      "With out having to write out the entire episode that has happened with this requester (I want to but I am at work currently), please be careful of this requester. He has been in here a couple times and in IRC. He is rude and thinks of us as the scum that does the work for him, and if you disagree, he will send you 3 emails to let you know how mad he is at you. What type of REQUESTER does this? None that I know of that are professional.\n",
      "Here is one of the emails I was sent. https://gyazo.com/81f9157b8cd2a38a23b226726f94fece\n",
      "Little background:\n",
      " \n",
      "* He came into IRC to ask our opinions on his hits, didn't like our answers, got mad and called us the lower workers (not exact words)\n",
      "* Came back with a new name and tried again in PMs. I talked to him for about 30 minutes. He said he would bonus me for helping him and even asked for my ID -Never received bonus, messaged him, no reply.\n",
      "* Went on TO, wrote a TRUTHFUL review of him and went about my day.\n",
      "* Got a bonus and a email from requester about 10 minutes later, saying he has bonused me BEFORE the review and felt like I black mailed him for a bonus. No sir, you offered, I wanted you to follow through.\n",
      "* I have now gotten 3 emails from him, including the one i have posted above.\n",
      " \n",
      "I wanted to post this, because talking and helping requester should be encouraged, because they need work done, and we want to make money. But this is an example of a requester thinking the lowest of us and treating us poorly, which is not ok.\n",
      "Please just be weary of doing hits for someone that does not trust us, and if you dont agree with them, will verbally attack you.  \n",
      "\n",
      "\n",
      "This requester showed up in IRC and wants me to post the rest of his emails, since he sent me 3 becuase I hurt his feelings. https://gyazo.com/4e6185c487434f738482cec71209b5a0\n",
      "\n",
      "Here is what the hit looks like:https://embed.gyazo.com/0d26ce8e82211ce0d19de5abe308f313.png\n",
      "\n",
      "He has changed his name a couple times. \n",
      "\n",
      "*************************\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ind=3\n",
    "samp_post=neg_sent_data.iloc[best_id[ind],:]\n",
    "print 'Component '+str(ind)\n",
    "print str(samp_post.raw_text)\n",
    "print '\\n*************************\\n'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And requester horror stories. This post in particular is quite detailed. Obviously, **requester** and **email** were key features. There's no subtle point to take away from this, just, act like a decent human being and treat people with respect. Don't play mindgames or form Ponzi schemes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Positive sentiment topic model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally let's see what people are speaking positively about by running an LDA on just the posts that were classified as positive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>topic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Component 0</th>\n",
       "      <td>hits</td>\n",
       "      <td>survey</td>\n",
       "      <td>approved</td>\n",
       "      <td>bonus</td>\n",
       "      <td>people</td>\n",
       "      <td>mturk</td>\n",
       "      <td>going</td>\n",
       "      <td>pay</td>\n",
       "      <td>make</td>\n",
       "      <td>today</td>\n",
       "      <td>complete</td>\n",
       "      <td>surveys</td>\n",
       "      <td>like</td>\n",
       "      <td>good</td>\n",
       "      <td>thanks</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Component 1</th>\n",
       "      <td>got</td>\n",
       "      <td>account</td>\n",
       "      <td>thanks</td>\n",
       "      <td>way</td>\n",
       "      <td>hits</td>\n",
       "      <td>amazon</td>\n",
       "      <td>earnings</td>\n",
       "      <td>ago</td>\n",
       "      <td>approved</td>\n",
       "      <td>qualification</td>\n",
       "      <td>script</td>\n",
       "      <td>reason</td>\n",
       "      <td>say</td>\n",
       "      <td>hello</td>\n",
       "      <td>mturk</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Component 2</th>\n",
       "      <td>hits</td>\n",
       "      <td>thanks</td>\n",
       "      <td>time</td>\n",
       "      <td>days</td>\n",
       "      <td>hit</td>\n",
       "      <td>mturk</td>\n",
       "      <td>day</td>\n",
       "      <td>good</td>\n",
       "      <td>new</td>\n",
       "      <td>guys</td>\n",
       "      <td>dont</td>\n",
       "      <td>scripts</td>\n",
       "      <td>work</td>\n",
       "      <td>really</td>\n",
       "      <td>got</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Component 3</th>\n",
       "      <td>amazon</td>\n",
       "      <td>account</td>\n",
       "      <td>hit</td>\n",
       "      <td>email</td>\n",
       "      <td>requester</td>\n",
       "      <td>mturk</td>\n",
       "      <td>turk</td>\n",
       "      <td>work</td>\n",
       "      <td>hits</td>\n",
       "      <td>contact</td>\n",
       "      <td>new</td>\n",
       "      <td>like</td>\n",
       "      <td>payments</td>\n",
       "      <td>help</td>\n",
       "      <td>use</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  0        1         2      3          4       5         6  \\\n",
       "Component 0    hits   survey  approved  bonus     people   mturk     going   \n",
       "Component 1     got  account    thanks    way       hits  amazon  earnings   \n",
       "Component 2    hits   thanks      time   days        hit   mturk       day   \n",
       "Component 3  amazon  account       hit  email  requester   mturk      turk   \n",
       "\n",
       "                7         8              9        10       11        12  \\\n",
       "Component 0   pay      make          today  complete  surveys      like   \n",
       "Component 1   ago  approved  qualification    script   reason       say   \n",
       "Component 2  good       new           guys      dont  scripts      work   \n",
       "Component 3  work      hits        contact       new     like  payments   \n",
       "\n",
       "                 13      14  topic  \n",
       "Component 0    good  thanks      0  \n",
       "Component 1   hello   mturk      1  \n",
       "Component 2  really     got      2  \n",
       "Component 3    help     use      3  "
      ]
     },
     "execution_count": 323,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_fname='pos_lda.pkl'\n",
    "pos_sent_data=all_data.loc[all_data.sentiment=='positive',:]\n",
    "pos_sent_text=pos_sent_data.proc_text2\n",
    "if not os.path.exists(pos_fname):\n",
    "\n",
    "    n_features=100\n",
    "    tf_vectorizer = CountVectorizer(min_df=5, max_features=n_features,stop_words='english')\n",
    "    pos_tf = tf_vectorizer.fit_transform(pos_sent_text)\n",
    "\n",
    "    # Fit LDA on all test\n",
    "    pos_lda_model=LatentDirichletAllocation(n_topics=4)\n",
    "    pos_lda_model.fit(pos_tf);\n",
    "    pos_tf_feature_names = tf_vectorizer.get_feature_names()\n",
    "    \n",
    "    with open(pos_fname,'wb') as post_file:\n",
    "        pk.dump((pos_lda_model,pos_tf_feature_names,pos_tf),post_file)\n",
    "else:\n",
    "    with open(pos_fname,'rb') as post_file:\n",
    "        pos_lda_model,pos_tf_feature_names,pos_tf=pk.load(post_file)\n",
    "\n",
    "pos_top_features=get_top_features(pos_lda_model,pos_tf_feature_names,15,verbose=False)   \n",
    "pos_top_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pos_sent_data.loc[:,'topic']=[np.argmax(i) for i in pos_lda_model.transform(pos_tf)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "topic\n",
       "0    63\n",
       "1    17\n",
       "2    96\n",
       "3    36\n",
       "dtype: int64"
      ]
     },
     "execution_count": 325,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_sent_data.groupby('topic').size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "topics=np.unique(pos_sent_data.loc[:,'topic'])\n",
    "weights=pos_lda_model.transform(pos_tf)\n",
    "norm=repmat([ [i] for i in np.sum(weights,axis=1)],1,len(topics))\n",
    "best_id=np.argmax(weights/norm,axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topic 0: Your friendly, neighborhood Requestor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Component 0\n",
      "Hello, I'll be posting a survey on MTurk for the first time within the next ten days. The survey itself will be approximately 30 items long and will take approximately 2.5-5 minutes to complete. I'm going to need approximately 500 people to complete this survey and plan on paying my turkers $.75 for this task. \n",
      "\n",
      "However, I am concerned about the potential for inattentive respondents. Given that I am trying to examine the psychometric properties of these survey items for subsequent research, inattentive responding (i.e., Christmas-treeing the survey) would really throw a wrench in my work. \n",
      "\n",
      "1) Is $.75 enough to induce people to respond attentively for a 2.5-5 minute survey?\n",
      "\n",
      "2) I am planning on incorporating quality control items (e.g., \"Please select \"slightly agree\") in order to detect inattentive respondents. I'm also considering flagging people who complete the survey far too quickly ( < 1 minute or so). Are there any other methods commonly used in the turk community to detect inattentive respondents? \n",
      "\n",
      "3) Are masters qualifications effective for increasing overall respondent quality?\n",
      "\n",
      "4) As a rough estimate, approximately how long do you think it will take to collect 500 respondents with these parameters?\n",
      "\n",
      "Thank you turks, both mechanical and organic. \n",
      "\n",
      "\n",
      "\n",
      "*************************\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ind=0\n",
    "samp_post=pos_sent_data.iloc[best_id[ind],:]\n",
    "print 'Component '+str(ind)\n",
    "print str(samp_post.raw_text)\n",
    "print '\\n*************************\\n'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Coming off of the preceding horror story, this is the kind of requestor you should be--Concerned about the quality of your data, aware that you are paying people a small amount to do what is probably a boring task and the sense of humor of a sitcom parents from the 50's."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topic 1: Stories of forgiveness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Component 1\n",
      "I've been a turker since 2009, when I first turned 18. I'm a masters worker with 98% approval rating. At least, I was a turker until yesterday afternoon. Yesterday amazon suspended my account out of the blue, stating blocks as the reason.\n",
      "\n",
      "I've had one hard block in the last 6 months: John Wilson blocked me back in February. I've had 2 \"soft\" blocks since then, with no account warnings. \n",
      "\n",
      "One came from S. Eckard, who also blocked another worker on turkopticon. And the other one came from Jacob Dink. I must have gotten another soft block yesterday, though I don't know from who. I never got an account warning, so it had to be a soft block. But amazon took it upon themselves to suspend my account.\n",
      "\n",
      "This is just a heads up to my fellow turkers: soft blocks DO count against you. So be careful out there!\n",
      "\n",
      "Edit: Just a quick update. I sent three messages. On the first one, I got an automated message and then a real live human saying they would sent my case to the account suspension review team. No response after that. The second, I got an automated message, but no human follow up. Third, no response at all, but I asked about getting my account money out. They must have heard me, because today I got a message saying that my earnings transfer was complete. I'm happy that I got my money back at least. But I'm thinking about emailing Jeff Bezos...maybe it'll do some good. \n",
      "\n",
      "*************************\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ind=1\n",
    "samp_post=pos_sent_data.iloc[best_id[ind],:]\n",
    "print 'Component '+str(ind)\n",
    "print str(samp_post.raw_text)\n",
    "print '\\n*************************\\n'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When I first started reading this post, I thought \"Omg my sentiment classifier sucks. But I got to the end and the moral of this topic seems to be, \"People on the internet make mistakes and *sometimes* we fix them\". Note the emphasis on the *sometimes*; this is the rarest category. The LDA feature **thanks** is probably pretty diagnostic of that."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topic 2: Spaceballs 3: The Search for Even More Money"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Component 2\n",
      "I've been turking for about 3 months now and recently started using hitscraper. I've definitely noticed an increase in my pay. I've been able to grab several very juicy hits that I doubt I would've gotten otherwise. My question: is there a way to make hitscraper auto accept hits? The best ones that come in I can only seem to get about 25% of the time. This is while I'm working on a hit in one tab I react to the beep and try to PANDA the new good one. If I don't get it, I put it into turkmaster and auto accept it. This ends yielding me the hit about 10% of the time. How should I use these two scripts together? I mostly do surveys. Can never seem to find profitable batches. \n",
      "\n",
      "*************************\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ind=2\n",
    "samp_post=pos_sent_data.iloc[best_id[ind],:]\n",
    "print 'Component '+str(ind)\n",
    "print str(samp_post.raw_text)\n",
    "print '\\n*************************\\n'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yup, this post came up again! This time it seems to be representing some sort of Horatio Alger topic about how to get more HITs in less **time**/**days**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topic 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Component 3\n",
      "I received an e-mail from Amazon about a block from Lab 322. Did anyone else receive this? I'm still pretty new (started on MTurk Jan 2) and not sure what, if anything, to do. I went through my spreadsheet and I've done two HITs for Lab 322, on March 2 and 11, and both with the same name (but different pay amounts). I don't remember what the surveys were about, but I wonder if they were the same surveys and that was why I was blocked.\n",
      "\n",
      "Here's the e-mail:\n",
      "\n",
      "Greetings from Mechanical Turk.\n",
      "\n",
      "We regret to inform you that you were blocked from working on HITs by the following requester(s):\n",
      "\n",
      "LAB 322\n",
      "\n",
      "Requesters typically block Workers who submit poor quality work.\n",
      "\n",
      "Should additional requesters block you in the next few months, we may suspend your account.\n",
      "\n",
      "Regards,\n",
      "The Mechanical Turk Team\n",
      "\n",
      "Any help on what to do is appreciated. Thank you!\n",
      "\n",
      "ETA: Sent message, received answer: \"we found it to be of poor quality and ultimately unusable for our analyses\". I take my time with all the surveys I do, giving thought to the answers I give so I can do good work. Oh, well, I guess that's that.\n",
      "\n",
      "*************************\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ind=3\n",
    "samp_post=pos_sent_data.iloc[best_id[ind],:]\n",
    "print 'Component '+str(ind)\n",
    "print str(samp_post.raw_text)\n",
    "print '\\n*************************\\n'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, this topic is something of a weird subset of people who politely asked for help **AND** also copied and pasted Amazon emails, as reflected by the most diagnostic word being **amazon**...let's not read too much into this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Negative posts in depth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This first round of sentiment classification and topic modeling has given us a broad overview of what people are talking about on the mturk subreddit. Sometimes people complain about a HIT, specific requesters, or some technical issue they have with Amazon.\n",
    "\n",
    "Building on that, can we get any more specific tips for requesters, above and beyond \"Don't be a jerk and debug your code?\". I'm also going to try to give specific advice in response to these topics based on my own experience writing mturk studies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get the subset of negative posts about task errors (neg_sent_data.topic==1) and requesters (neg_sent_data.topic==3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "requester_issue_data=neg_sent_data.loc[np.logical_or(neg_sent_data.topic==1,neg_sent_data.topic==3)].reset_index()\n",
    "requester_issue_text=requester_issue_data.proc_text2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>topic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Component 0</th>\n",
       "      <td>hit</td>\n",
       "      <td>hits</td>\n",
       "      <td>time</td>\n",
       "      <td>accept</td>\n",
       "      <td>way</td>\n",
       "      <td>available</td>\n",
       "      <td>know</td>\n",
       "      <td>click</td>\n",
       "      <td>mturk</td>\n",
       "      <td>dont</td>\n",
       "      <td>requester</td>\n",
       "      <td>thanks</td>\n",
       "      <td>question</td>\n",
       "      <td>turking</td>\n",
       "      <td>working</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Component 1</th>\n",
       "      <td>requester</td>\n",
       "      <td>hit</td>\n",
       "      <td>got</td>\n",
       "      <td>hits</td>\n",
       "      <td>rejection</td>\n",
       "      <td>like</td>\n",
       "      <td>rejected</td>\n",
       "      <td>didnt</td>\n",
       "      <td>wrong</td>\n",
       "      <td>sure</td>\n",
       "      <td>said</td>\n",
       "      <td>survey</td>\n",
       "      <td>thanks</td>\n",
       "      <td>mturk</td>\n",
       "      <td>help</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Component 2</th>\n",
       "      <td>turk</td>\n",
       "      <td>bad</td>\n",
       "      <td>ago</td>\n",
       "      <td>started</td>\n",
       "      <td>really</td>\n",
       "      <td>mechanical</td>\n",
       "      <td>like</td>\n",
       "      <td>things</td>\n",
       "      <td>got</td>\n",
       "      <td>good</td>\n",
       "      <td>say</td>\n",
       "      <td>account</td>\n",
       "      <td>hits</td>\n",
       "      <td>sorry</td>\n",
       "      <td>know</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Component 3</th>\n",
       "      <td>hits</td>\n",
       "      <td>hit</td>\n",
       "      <td>page</td>\n",
       "      <td>says</td>\n",
       "      <td>mturk</td>\n",
       "      <td>task</td>\n",
       "      <td>search</td>\n",
       "      <td>trying</td>\n",
       "      <td>problem</td>\n",
       "      <td>try</td>\n",
       "      <td>new</td>\n",
       "      <td>script</td>\n",
       "      <td>qualification</td>\n",
       "      <td>getting</td>\n",
       "      <td>working</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Component 4</th>\n",
       "      <td>account</td>\n",
       "      <td>email</td>\n",
       "      <td>hit</td>\n",
       "      <td>information</td>\n",
       "      <td>submit</td>\n",
       "      <td>click</td>\n",
       "      <td>bonus</td>\n",
       "      <td>requester</td>\n",
       "      <td>work</td>\n",
       "      <td>turk</td>\n",
       "      <td>message</td>\n",
       "      <td>got</td>\n",
       "      <td>sent</td>\n",
       "      <td>link</td>\n",
       "      <td>mechanical</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Component 5</th>\n",
       "      <td>tried</td>\n",
       "      <td>hit</td>\n",
       "      <td>qualified</td>\n",
       "      <td>turkmaster</td>\n",
       "      <td>email</td>\n",
       "      <td>work</td>\n",
       "      <td>today</td>\n",
       "      <td>got</td>\n",
       "      <td>times</td>\n",
       "      <td>getting</td>\n",
       "      <td>edit</td>\n",
       "      <td>new</td>\n",
       "      <td>know</td>\n",
       "      <td>ago</td>\n",
       "      <td>sure</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Component 6</th>\n",
       "      <td>like</td>\n",
       "      <td>time</td>\n",
       "      <td>make</td>\n",
       "      <td>way</td>\n",
       "      <td>sorry</td>\n",
       "      <td>rejections</td>\n",
       "      <td>survey</td>\n",
       "      <td>dont</td>\n",
       "      <td>questions</td>\n",
       "      <td>workers</td>\n",
       "      <td>work</td>\n",
       "      <td>hours</td>\n",
       "      <td>thanks</td>\n",
       "      <td>help</td>\n",
       "      <td>mturk</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Component 7</th>\n",
       "      <td>hits</td>\n",
       "      <td>dont</td>\n",
       "      <td>work</td>\n",
       "      <td>day</td>\n",
       "      <td>like</td>\n",
       "      <td>requesters</td>\n",
       "      <td>pay</td>\n",
       "      <td>mturk</td>\n",
       "      <td>want</td>\n",
       "      <td>good</td>\n",
       "      <td>batch</td>\n",
       "      <td>surveys</td>\n",
       "      <td>getting</td>\n",
       "      <td>really</td>\n",
       "      <td>hit</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     0      1          2            3          4           5  \\\n",
       "Component 0        hit   hits       time       accept        way   available   \n",
       "Component 1  requester    hit        got         hits  rejection        like   \n",
       "Component 2       turk    bad        ago      started     really  mechanical   \n",
       "Component 3       hits    hit       page         says      mturk        task   \n",
       "Component 4    account  email        hit  information     submit       click   \n",
       "Component 5      tried    hit  qualified   turkmaster      email        work   \n",
       "Component 6       like   time       make          way      sorry  rejections   \n",
       "Component 7       hits   dont       work          day       like  requesters   \n",
       "\n",
       "                    6          7          8        9         10       11  \\\n",
       "Component 0      know      click      mturk     dont  requester   thanks   \n",
       "Component 1  rejected      didnt      wrong     sure       said   survey   \n",
       "Component 2      like     things        got     good        say  account   \n",
       "Component 3    search     trying    problem      try        new   script   \n",
       "Component 4     bonus  requester       work     turk    message      got   \n",
       "Component 5     today        got      times  getting       edit      new   \n",
       "Component 6    survey       dont  questions  workers       work    hours   \n",
       "Component 7       pay      mturk       want     good      batch  surveys   \n",
       "\n",
       "                        12       13          14  topic  \n",
       "Component 0       question  turking     working      0  \n",
       "Component 1         thanks    mturk        help      1  \n",
       "Component 2           hits    sorry        know      2  \n",
       "Component 3  qualification  getting     working      3  \n",
       "Component 4           sent     link  mechanical      4  \n",
       "Component 5           know      ago        sure      5  \n",
       "Component 6         thanks     help       mturk      6  \n",
       "Component 7        getting   really         hit      7  "
      ]
     },
     "execution_count": 414,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ri_fname='requester_issue_data.pkl'\n",
    "\n",
    "if not os.path.exists(ri_fname):\n",
    "\n",
    "    n_features=100\n",
    "    tf_vectorizer = CountVectorizer(min_df=5, max_features=n_features,stop_words='english')\n",
    "    ri_tf = tf_vectorizer.fit_transform(requester_issue_text)\n",
    "\n",
    "    # Fit LDA on all test\n",
    "    ri_lda_model=LatentDirichletAllocation(n_topics=8)\n",
    "    ri_lda_model.fit(ri_tf);\n",
    "    ri_tf_feature_names = tf_vectorizer.get_feature_names()\n",
    "    \n",
    "    with open(ri_fname,'wb') as post_file:\n",
    "        pk.dump((ri_lda_model,ri_tf_feature_names,ri_tf),post_file)\n",
    "else:\n",
    "    with open(ri_fname,'rb') as post_file:\n",
    "        ri_lda_model,ri_tf_feature_names,ri_tf=pk.load(post_file)\n",
    "\n",
    "ri_top_features=get_top_features(ri_lda_model,ri_tf_feature_names,15,verbose=False)   \n",
    "ri_top_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And let's look at these components in more detail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "requester_issue_data.loc[:,'topic']=[np.argmax(i) for i in ri_lda_model.transform(ri_tf)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "topic\n",
       "0    49\n",
       "1    59\n",
       "2    15\n",
       "3    69\n",
       "4    22\n",
       "5    19\n",
       "6    42\n",
       "7    58\n",
       "dtype: int64"
      ]
     },
     "execution_count": 416,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "requester_issue_data.groupby('topic').size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "topics=np.unique(requester_issue_data.loc[:,'topic'])\n",
    "weights=ri_lda_model.transform(ri_tf)\n",
    "norm=repmat([ [i] for i in np.sum(weights,axis=1)],1,len(topics))\n",
    "best_id=np.argmax(weights/norm,axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic 0: Make sure people can accept your HIT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Component 0\n",
      "This is the third time I've tried to do a HIT by this requester since I started turking, and for the third time I can't submit it! (The HIT about searching youtube)\n",
      "\n",
      "People absolutely love this guy and lavish mountains of praise about him on TO, but I can NEVER get his HITs to work :( I've already wasted over 20 minutes so it's no longer worth the $2.\n",
      "\n",
      "So, is it just me that can never seem to do a SS HIT?\n",
      "\n",
      "(I was able to do the join a group one, but it had no form so I don't really count it)\n",
      "\n",
      "*************************\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ind=0\n",
    "samp_post=requester_issue_data.iloc[best_id[ind],:]\n",
    "print 'Component '+str(ind)\n",
    "print str(samp_post.raw_text)\n",
    "print '\\n*************************\\n'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on this post and the features **accept** (3) and **click** (7), as a requester make sure people can **Accept** and **Submit** your HIT. \n",
    "\n",
    "Some possible reasons this wouldn't work:\n",
    "\n",
    "* You have a check on who can accept your HIT (e.g., using Chrome, not a repeat subject) which is either not working or isn't obvious to your subjects\n",
    "* You have a button with a link to a page with a typo or filepath that you changed\n",
    "* You didn't submit the HIT with an assignmentId\n",
    "* You left the submit link set to sandbox mode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topic 1: Rejection hurts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Component 1\n",
      "I'll try to long story short. I just started doing mturk and haven't done many HITs (having trouble finding HITs I can do and also doing more than 3 or 4 an hour but that's another issue entirely). I did a table extraction from requester Recognos Dst and got rejected with this message \"Rejected. The tablewrong extracted.\" (this is copy pasted from the rejection msg).\n",
      "\n",
      "\n",
      "Now, I saw a lot of these table extraction jobs from them and looked at the files ahead of time, which maybe I'm doing something wrong but I couldn't find the info they wanted in 3 of the posts (of a specific type of table) and waited until I found one that had the EXACT header (there were many that had close headers and I didn't attempt those HITs). I only did one HIT from them because I am nervous of rejections right now since I am so new and want to spread around the types in case I encounter a problem.\n",
      "\n",
      "Well I woke up this morning to this rejection and I'm panicking because again, not many HITs right now and even though I've been trying to get more volume it's not happening. If my Accepted percent goes below 95 what am I going to do?????????\n",
      "\n",
      "I know there is the recommended letter in the FAQ and I guess I should send it just to see if I can re-try the HIT and get the rejection overturned but I feel like they are just going to say no. Any suggestions on tweaking the request to rescind the rejection or any other suggestions? I'm 100% sure I did a table with the correct title, so unless there was another table that wasn't labeled properly (which wouldn't be my fault but the requester would see my work as incorrect) I know I did everything right.\n",
      "\n",
      "Thank you all for your help, this is my only hope for some income right now and I'm a bit desperate.\n",
      "\n",
      "*************************\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ind=1\n",
    "samp_post=requester_issue_data.iloc[best_id[ind],:]\n",
    "print 'Component '+str(ind)\n",
    "print str(samp_post.raw_text)\n",
    "print '\\n*************************\\n'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The focus here is **rejection** (4). Rejection is a pain for mturkers and there are a good number of people trying to make a living on mturk. Do not reject people unless there is really good evidence they are doing the bare minimum (e.g., clicking the same button the entire time). \n",
    "\n",
    "Generally, try to design your tasks so that it is hard for people to do the lazy thing and minimize unpleasantness. This is more of a design or UX question, like if you're afraid subjects are going to press a single button then make sure they have to move the mouse between trials."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topic 2: ???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Component 2\n",
      "Sorry if it's in the FAQ, I can't seem to find any info.\n",
      "\n",
      "I started literally 2 days ago and I can't seem to get my numbers up.\n",
      "\n",
      "I can't seem to do the ones by A9 because I'm not from the US :(. So my question is, being from SPAIN, am I just doomed to do shitty receipt tasks? I did a couple ones but didn't take long to realize they aren't worth it.\n",
      "\n",
      "Maybe I just don't know how to search? But searching the \"ones I'm qualified for\" only returns poor results. 14 pages. Most of them receipts and other document transcription.\n",
      "\n",
      "Should I create a US account with a US friend info and start over? How long does it take to get accepted? I got the email that I was accepted 3 days ago and I don't even remember when I sent the appliance.\n",
      "\n",
      "Can I just change where I live? Anyone in the same situation? The threads with the nice hits almost always ask to be from the US.\n",
      "\n",
      "Sorry for the bad wording, my redacting skills sucks + english no mother language + nap time in spain.\n",
      "\n",
      "*************************\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ind=2\n",
    "samp_post=requester_issue_data.iloc[best_id[ind],:]\n",
    "print 'Component '+str(ind)\n",
    "print str(samp_post.raw_text)\n",
    "print '\\n*************************\\n'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, I'm not sure what this one is about. I'm having difficulty combining the post and the key words into a coherent message. It's also the smallest category which may mean it's a bit of a hodgepodge. The most representative post itself is concerned about regional qualifications which segues nicely into..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topic 3: Qualifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Component 3\n",
      "I have been having an issue lately. I can take a qualification test or be granted a qualification and when I go to do the hit, I meet all the qualifications but it says I can't do the hit.\n",
      "\n",
      "I reached out to mturk, but of course, got nothing. They told me to contact the requester. This has happened with 3 different requesters so far.\n",
      "\n",
      "Also, yesterday, before the issue with mTurk not working, a requester told me not to do anymore of their hits in that batch because the qualifications that had put to prevent retakes wasn't working. They said that I wasn't the only one that could do the hits even though there was a qual in place to prevent this.\n",
      "\n",
      "Here is a screenshot of one of the hits I can't do: http://imgur.com/gcroyX1\n",
      "\n",
      "*************************\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ind=3\n",
    "samp_post=requester_issue_data.iloc[best_id[ind],:]\n",
    "print 'Component '+str(ind)\n",
    "print str(samp_post.raw_text)\n",
    "print '\\n*************************\\n'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This component seems like it might concern qualifications for HITs (feature word **qualification** (12)). Frankly, I rarely use qualifications (except for making sure turkers are from the US if that's important to my task).\n",
    "\n",
    "My advice is keep an eye on the qualifications you have set up for your HIT. Sometimes these will be important for your study, sometimes things will wash out. As a psychologist, I tend to like studying effects that are robust even amongst diverse populations doing my study through the internet. And with good experiment design (see topic 2), you don't need qualifications that weed out turkers with poor ratings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topic 4: Check your email and compensate your turkers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Component 4\n",
      "Have you ever had a requester say they sent you a bonus, but you never got an email? How can the requester verify it was sent? If they send tons of bonuses and/or post tons of hits, they might not notice the money being debited for one specific user, so how can they figure it out?\n",
      "\n",
      "This came up today from a worker in IRC, and it got me thinking. There's got to be a way to verify bonus payments as a requester, since the transactions don't show up in your account. Well, after some digging, I figured it out:\n",
      "\n",
      "1. Go to the [manage](https://requester.mturk.com/manage) site\n",
      "1. Click on [Manage Hits Individually](https://i.gyazo.com/2f9be62575414fc6f9116d43dbaddad9.png)\n",
      "1. Click on [Download Results](https://i.gyazo.com/2515a1475045d634d82ce60aacb1419c.png) for the hit you wish to verify.\n",
      "1. Click on [Approved](https://i.gyazo.com/f333b301b71dc495929bdcaed081c3ae.png) to view all the approvals for that hit\n",
      "1. Search for the worker ID with ctrl+f\n",
      "\n",
      "If the worker has been bonused, you will see a [little star with the amount](https://i.gyazo.com/e17bee854392a5876644decec48d0ed0.png). If not, there will be nothing. You can use this interface to send a bonus and/or to send that worker an email as well, just click the [little arrow](https://i.gyazo.com/306d57f013aa6b7dc69e8d9a7a79f271.gif) that appears when you mouseover the ID.\n",
      "\n",
      "ETA: Sending an email from this interface will send from your mturk account email address, to the worker's mturk account email address, and will contain the footer:\n",
      "    \n",
      "    Greetings from Amazon Mechanical Turk,\n",
      "    \n",
      "    The message above was sent by an Amazon Mechanical Turk user.    \n",
      "    Please review the message and respond to it as you see fit.\n",
      "\n",
      "    Sincerely,\n",
      "    Amazon Mechanical Turk\n",
      "    https://www.mturk.com\n",
      "\n",
      "It also contains at the top: \n",
      "`Message from <Requester account name> (<requester account email address>)`\n",
      "obviously with the bracketed information replaced.\n",
      "\n",
      "Hopefully this will be helpful for someone. Obviously this depends on the requester being a good person and actually wanting to give you the bonus instead of just someone who's lying about it, but at least this will give you some screenshots and explanation to show them so they can see.\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "EDIT: There is a way to see it on your transaction list I found.\n",
      "\n",
      "1. Go to [Your Account](https://requester.mturk.com/account)\n",
      "1. Click [Transaction History](https://i.gyazo.com/3e4511fc9240f550a0666535838cd4a6.png)\n",
      "1. Find the [Payments to workers](https://i.gyazo.com/965cab7611e6b83935d418375fbb9901.png) for the *date that you granted the bonus* and click it\n",
      "1. Using ctrl+f, search for the [worker ID](https://i.gyazo.com/b5b220b50a6da95e49c6e3941401d55a.png) of the worker who is requesting bonus verification (or who you want to see), and click the ID.\n",
      "1. All approvals, including the [bonus verification](https://i.gyazo.com/20ca62bc6c94f67e0687024ceaaa0975.png), will be listed on that page for that date. Including transaction ID, which can be used to positively verify with Amazon.\n",
      "\n",
      "This method is a little more definitive, since it shows the money leaving your account, and you can't fake that transaction ID. Though I haven't tested it, I'm sure that can be tracked somewhere with Amazon for third-party verification.\n",
      "\n",
      "*************************\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ind=4\n",
    "samp_post=requester_issue_data.iloc[best_id[ind],:]\n",
    "print 'Component '+str(ind)\n",
    "print str(samp_post.raw_text)\n",
    "print '\\n*************************\\n'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On mturk, you might give bonuses (key word **bonus** (6)) because:\n",
    "\n",
    "#### You want to motivate good performance\n",
    "\n",
    "Giving performance based bonuses are a good way to motivate turkers. Good design can only go so far in encouraging people to do an unavoidably dull task well. If you've got the time and funds, give a little monetary incentive (though, [as this excellent study shows from Crump, McDonnell and Gureckis (2013), ](http://journals.plos.org/plosone/article?id=10.1371/journal.pone.0057410) certain studies benefit more from extra cash).\n",
    "\n",
    "#### You messed up and people are yelling at you \n",
    "\n",
    "Your code gone goofed up and now your inbox (key word **email** (1)) is full of angry emails. Since in many cases these subjects weren't able to complete your HIT, I have a separate HIT that I can direct people to where I can give them a bonus to compensate them. Make sure you're courteous in the email--as these analyses have shown, mturkers talk to each other\n",
    "\n",
    "\n",
    "Note, this post is a little odd. I don't entirely get the gist of it; it seems to deal more with some internal system minutiae I'm not familiar with"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topic 5: More qualification issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Component 5\n",
      "I just got an email while I was attempting to work on a UserBob HIT and it said my qualification was revoked. I hadn't even completed the HIT, so I'm not sure what's up...? Has this happened to anyone else? I was kind of disappointed since I just qualified for his HITs two or three days ago and I haven't really gotten an opportunity to do any until today.\n",
      "\n",
      "*************************\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ind=5\n",
    "samp_post=requester_issue_data.iloc[best_id[ind],:]\n",
    "print 'Component '+str(ind)\n",
    "print str(samp_post.raw_text)\n",
    "print '\\n*************************\\n'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This seems like more issues with qualifications (Topic 3), except maybe with the addendum that people were notified through email"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topic 6: Time==Money"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Component 6\n",
      "Hey guys,\n",
      "\n",
      "I've been browning this subreddit and all the other ones listed on the sidebar. Of course, I also read the FAQ. There are just a few things that are kind of unclear for me so I thought I would ask and see if anyone wanted to answer. I appreciate anyone who reads this and takes the time to answer. I'm sure it's annoying when new people pop up and want to ask questions. \n",
      "\n",
      "- Am I understanding properly that you can only transfer $500 per month to a bank account?\n",
      "\n",
      "- My general goal for this is to make extra cash while I'm at my IRL job. The amount per day, or even hour, doesn't need to be consistent but I'm thinking maybe $150 per week. I work 30 hours a week and the majority of my time is absolutely free. My boss has no problem with me doing online work. He suggested it since I'm not enrolled in school. (Not Mturk - found it through r/beermoney).. Is this amount doable? \n",
      "\n",
      "- Probation period is 10 days, you have to do 3 hits a day, correct? \n",
      "\n",
      "- What are qualifications and how do I get them? ELI5, please. For some reason it's going over my head. \n",
      "\n",
      "Any other tips or general info, I would appreciate. I have no problem doing the really low-paying and mindless tasks. I'm not trying to sound ungrateful because I LOVE my job but there are very extended periods of time (1-2 hours) where I literally do nothing but watch Netflix. I am using Mturk as a way to make cash and kill time. \n",
      "\n",
      "Also, if there are any other websites or online work that people do to make cash? I'm trying to go through r/beermoney and r/workonline but it's a bit overwhelming. I signed up for Freelancer but it seems like everything on there is a scam...? \n",
      "\n",
      "Thanks guys, I seriously appreciated.\n",
      "\n",
      "TL;DR I'm an idiot and I dont know what I'm doing. \n",
      "\n",
      "*************************\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ind=6\n",
    "samp_post=requester_issue_data.iloc[best_id[ind],:]\n",
    "print 'Component '+str(ind)\n",
    "print str(samp_post.raw_text)\n",
    "print '\\n*************************\\n'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These posts seem to be concerned about ways to make the most money through mturk (key words **time** (1), **make** (2), **way** (3)). In general, make sure you pay turkers fairly. I usually aim for something like $4 per hour (plus or minus how enjoyable the task is). Try to say on the very first page in **LARGE BOLD FONT** how much money you will pay and how long the task will be"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topic 7: Finding good HITs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Component 7\n",
      "I'm not really able to do work outside my home atm. I'm newly disabled and still learning my limitations. I'm trying to make at LEAST $10 a day, but that's not really a lot when you're going to need to pay rent at some point. I'm still under 1000 HITS but above 500. Fell into some of the newb traps like p9r during my probation period to get my HITS up, but thankfully no random rejections. \n",
      "At this point it seems like my life is nothing but copying receipts, and I'm trying to find the most efficient requesters for the money atm. I'm spreadsheeting my time and earnings from different requesters, to see who I'm doing that pays best and within a reasonable amount of time, but I'm kind of sick of penny batching that is actually unfair amounts of work requested for the time it takes. \n",
      "\n",
      "I'm using turkopticon, but I'm having problems with installing any of the other scripts, and mostly I'm keeping window tabs of HITS from my preferred requesters open at all times, but even then, the \"good\" requesters I don't meet quals for, or can't do the work for period.  I'm mostly just frustrated, and need some better advice on how to make actual money here. I'm trying my best on my own but I think I do need the push for how to use the tools at this point. Chrome isn't wanting me to install scripts, java isn't compiling right and I have no idea how to fix it.  \n",
      "\n",
      "TL;DR Newbie turker needs help with scripts and better requesters\n",
      "\n",
      "*************************\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ind=7\n",
    "samp_post=requester_issue_data.iloc[best_id[ind],:]\n",
    "print 'Component '+str(ind)\n",
    "print str(samp_post.raw_text)\n",
    "print '\\n*************************\\n'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is more about mturkers trying to find good HITs, nothing much that hasn't been covered in previous topics. This particular post does mention [Turkopticon](https://turkopticon.ucsd.edu/) though, which, if you're a requester who's not aware, is where all the mturkers go to rate you. If no one is doing your HIT, go to Turkopticon and see what your reputation looks like--a buggy HIT or misinterpreted/unread email may be dragging down your score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, we were able to look at how mturkers are feeling on mturk and what they're talking about. And, in response to some of the more negative comments, I hope I was able to give some useful advice from my own experience about how to make better studies and interact with mturkers civilly :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To recap:\n",
    "\n",
    "* Don't go around rejecting people en masse; try improving experiment design instead\n",
    "* Make sure people can Accept and Submit your HITs\n",
    "* Experiment design can also help obviate the need for qualifications\n",
    "* Bonuses are good motivators and a way to make up for mistakes\n",
    "* Be upfront about task pay and length\n",
    "* Be aware of your reputation (mturkers are organized!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the links I mentioned throughout this:\n",
    "\n",
    "* [Mechanical Turk](https://www.mturk.com/mturk/welcome)\n",
    "* [mturk subreddit](https://www.reddit.com/r/mturk/)\n",
    "* [Hits Worth Turking For subreddit](https://www.reddit.com/r/HITsWorthTurkingFor/)\n",
    "* [Turkopticon](https://turkopticon.ucsd.edu/). It also has a Chrome extension!\n",
    "* [Crump, McDonnell and Gureckis (2013)](http://journals.plos.org/plosone/article?id=10.1371/journal.pone.0057410)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
